---
title: "Assignment No.3"
author: "Elior Bliah"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: kable
    results: hide
    theme: flatly
    toc: yes
  word_document:
    toc: yes
---

# Preface

## Open Q1

## Open Q2


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r loading, warning=FALSE, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, magrittr, tidymodels, caret, DALEX, rpart, rattle, rpart.plot, RColorBrewer, ada, doParallel, pROC)
```

```{r}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
```

```{r load wine_data+split, include=FALSE}
urlfile= "https://raw.githubusercontent.com/ml4econ/problem-sets-2020/master/datasets/winequality_red.csv"
winequality_red<- read_csv(url(urlfile))
set.seed(167)
wine_split<- initial_split(winequality_red)
wine_train<- training(wine_split)
wine_test<- testing(wine_split)

```

```{r load heart_data+split, include=FALSE}
urlfile= "https://raw.githubusercontent.com/ml4econ/problem-sets-2020/master/datasets/heart.csv"
hearts<- read_csv(url(urlfile))
set.seed(167)
hearts_split<- initial_split(hearts, prop=0.7)
hearts_split
heart_train<- training(hearts_split)
heart_test<- testing(hearts_split)
```

# Trees

## Open Q1


```{r}
formula_part <- target ~ sex + cp + chol
formula_full <- target ~ .
```

```{r reducedFtree}
reducedtree<- rpart(formula_part, data = heart_train , method = "class")
fancyRpartPlot(reducedtree, caption = NULL)
```
```{r twomodels_setting}
# first model output
firstModel<- rpart(formula = formula_full, data = heart_train, method = "class", minsplit = 2, minbucket = 1)
printcp(firstModel)
# second model output
secondModel<- rpart(formula = formula_full, data = heart_train, method = "class")
printcp(secondModel)
```

In the first model we used 9 variables and in the second we used 4 variables.

## Predictions & confusionMatrix

```{r}
predC<- predict(firstModel, heart_train, type = "class")

train_tab <- table(predC, heart_train$target)
train_tab %>%
  confusionMatrix()


predA<- predict(secondModel, heart_train, type = "class")

train_tab <- table(predA, heart_train$target)
train_tab %>% 
  confusionMatrix()


predD<- predict(firstModel, heart_test, type = "class")

train_tab <- table(predD, heart_test$target)
train_tab %>% 
  confusionMatrix()

  
predB<- predict(secondModel, heart_test, type = "class")

train_tab <- table(predB, heart_test$target)
train_tab %>% 
  confusionMatrix()

```

We can see from the results that from both models the train set predict better than the the test set. that may be due to the greater amount of observation that the train set has. 

## Pruning
```{r Pruning}
thirdModel<- rpart(formula = formula_full, data = heart_train, method = "class", cp = 0.03)

predE<-predict(thirdModel, heart_train, type = "class")

train_tab <- table(predE, heart_train$target)
train_tab %>% 
  confusionMatrix()

predF<-predict(thirdModel, heart_test, type = "class")

train_tab <- table(predF, heart_test$target)
train_tab %>% 
  confusionMatrix()
```

We can see that the accuracy is very simmilar. that mean when the complexity parameter is close to zero we have a fully saturated tree. In our case, without any restrictions we force the model (we possibly overfit it) we acheive an accuary of 80% of predicting heart attack.