---
title: "Assignment No.3"
author: "Elior Bliah"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: kable
    results: hide
    theme: cerulean
    toc: TRUE
    toc_float: TRUE
---

# Preface

### Open Q1
Question: Why we are not tuning parameters in OLS or Logistic Regression? How do we estimate these models?

Answer: we are testing several models, looking for the best one whereas we are defining only one model in OLS.

### Open Q2

Question: Can we infer causality from interpretability?

Answer: ML models often lack interpretability, it may seem that these models are black boxes. There are multiple reasons why interpretability is important: from auditing to acceptance and even pure curiosity . From economists point of view it may have even greater importance - we canâ€™t infer causality from a black box process.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r loading, warning=FALSE, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, magrittr,randomForest, tidymodels, caret, DALEX, rpart, rattle, rpart.plot, RColorBrewer, ada, doParallel, pROC, DAELX)
```

```{r}
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
```

```{r load wine_data+split, include=FALSE}
urlfile= "https://raw.githubusercontent.com/ml4econ/problem-sets-2020/master/datasets/winequality_red.csv"
winequality_red<- read_csv(url(urlfile))
set.seed(167)
wine_split<- initial_split(winequality_red)
wine_train<- training(wine_split)
wine_test<- testing(wine_split)

```

```{r load heart_data+split, include=FALSE}
urlfile= "https://raw.githubusercontent.com/ml4econ/problem-sets-2020/master/datasets/heart.csv"
hearts<- read_csv(url(urlfile))
set.seed(167)
hearts_split<- initial_split(hearts, prop=0.7)
hearts_split
heart_train<- training(hearts_split)
heart_test<- testing(hearts_split)
```

## Trees

### Open Q1


```{r}
formula_part <- target ~ sex + cp + chol
formula_full <- target ~ .
```

```{r reducedFtree}
reducedtree<- rpart(formula_part, data = heart_train , method = "class")
fancyRpartPlot(reducedtree, caption = NULL)
```
```{r twomodels_setting}
# first model output
firstModel<- rpart(formula = formula_full, data = heart_train, method = "class", minsplit = 2, minbucket = 1)
printcp(firstModel)
# second model output
secondModel<- rpart(formula = formula_full, data = heart_train, method = "class")
printcp(secondModel)
```

In the first model we used 9 variables and in the second we used 4 variables.

## Predictions & confusionMatrix

```{r}
predC<- predict(firstModel, heart_train, type = "class")

train_tab <- table(predC, heart_train$target)
train_tab %>%
  confusionMatrix()


predA<- predict(secondModel, heart_train, type = "class")

train_tab <- table(predA, heart_train$target)
train_tab %>% 
  confusionMatrix()


predD<- predict(firstModel, heart_test, type = "class")

train_tab <- table(predD, heart_test$target)
train_tab %>% 
  confusionMatrix()

  
predB<- predict(secondModel, heart_test, type = "class")

train_tab <- table(predB, heart_test$target)
train_tab %>% 
  confusionMatrix()

```

We can see from the results that from both models the train set predict better than the the test set. that mean we have overtrain the model.

## Pruning
```{r Pruning}
thirdModel<- rpart(formula = formula_full, data = heart_train, method = "class", cp = 0.03)

predE<-predict(thirdModel, heart_train, type = "class")

train_tab <- table(predE, heart_train$target)
train_tab %>% 
  confusionMatrix()

predF<-predict(thirdModel, heart_test, type = "class")

train_tab <- table(predF, heart_test$target)
train_tab %>% 
  confusionMatrix()
```

We can see that the accuracy is very simmilar both in trainig and in the testing sets. that mean when the complexity parameter is close to zero we have a fully saturated tree. In our case, without any restrictions we force the model (we possibly overfit it) we acheive an accuary of 80% of predicting heart attack. that mean our model is good and we have trained well the model. NO OVERFITING

#  KNN , Random F, Boosting and bagging


```{r message=FALSE, warning=FALSE, include=FALSE}
set.seed(167)
heart_train$target %<>%  as.factor()
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3)

rf <- train(formula_full, data = heart_train, 
             method = "rf", 
             trControl = fitControl)

knn <- train(formula_full, data = heart_train, 
             method = "knn", 
             trControl = fitControl)

adam <- train(formula_full, data = heart_train, 
             method = "ada", 
             trControl = fitControl)
gbm <- train(formula_full, data = heart_train, 
             method = "gbm", 
             trControl = fitControl)


```

```{r}
p1 <-ggplot(rf)
p2 <-ggplot(knn)
p3 <- ggplot(adam)
p4 <- ggplot(gbm)

library(gridExtra)
grid.arrange(p1, p2, p3,p4, ncol = 2, nrow = 2)

```

## Interpertability

### explian

```{r}
heart_explain <- heart_train %>% select(-target)
heart_outcome <- as.numeric(as.character(heart_train$target))
rf_explainer <- explain(rf, label="rf", 
                         data = heart_explain,
                         y = heart_outcome)
```

### perfomance 

```{r}
set.seed(167)
mp_rf <- model_performance(rf_explainer)
plot(mp_rf)
plot(mp_rf, geom = "boxplot")

```

### model importance 
Answer : we are not able to compute other model since we didn't studied other model. however, we can see from the polts in the questions that the random forest model is the best because he has the lower rmse amd the lowest residuals. 

### Feature importance

```{r}
importance_rf <- variable_importance(rf_explainer)
plot(importance_rf)
```
Answer: we can see that if we drop the cp that the thal variables we will loss alot of information in our model.

### Partial dependence

```{r message=FALSE, warning=FALSE}
partial <- variable_effect(rf_explainer, colnames(heart_explain) , type = "partial_dependency")
plot(partial)
```

We can see that the 'thalach' and the 'chol' features are behaving differently in the knn model than other. 

### Break Dowm Profile

```{r}
new_cust <- heart_train[1, ] %>% as.data.frame()
new_cust_rf  <- predict_parts_break_down(rf_explainer, new_observation = new_cust)
plot(new_cust_rf)
```

we can see that in the random forest model the intercept influence most the total prediction. Afterward, the cp affect it next greatly. please note that this is only the
first row.

### Predict
```{r}
models_predictions <- data.frame(
  rf = predict(rf, newdata = heart_test),
  truth = heart_test$target
)
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
roc_rf <- roc(models_predictions$rf, heart_test$target) 

plot(roc_rf, col = "#00AEEF")
```

